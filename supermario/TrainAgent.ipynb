{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 16:03:27.857304: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 16:03:30.313052: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 16:03:30.319115: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 16:03:30.342676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 16:03:30.354905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n",
      "----START LEARNING----\n",
      "Logging to ./board/PPO-00003_2\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -inf - Last mean reward per episode: 1818.00\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 1818.00 - Last mean reward per episode: 1818.00\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 1818.0   |\n",
      "| time/              |          |\n",
      "|    fps             | 238      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 1818.00 - Last mean reward per episode: 2208.33\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 2208.33 - Last mean reward per episode: 2180.80\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.17e+03     |\n",
      "|    ep_rew_mean          | 2180.8       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 40           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 402          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0151887555 |\n",
      "|    clip_fraction        | 0.239        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -6.05        |\n",
      "|    explained_variance   | 0.844        |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 187          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.0021       |\n",
      "|    value_loss           | 314          |\n",
      "------------------------------------------\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 2208.33 - Last mean reward per episode: 2245.33\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2137.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.31e+03    |\n",
      "|    ep_rew_mean          | 2137.125    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 31          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 774         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016830266 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.02       |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 252         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00661     |\n",
      "|    value_loss           | 323         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2077.80\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2088.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.99e+03   |\n",
      "|    ep_rew_mean          | 2014.5625  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 29         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 1121       |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01813323 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -6         |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 437        |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | 0.00392    |\n",
      "|    value_loss           | 447        |\n",
      "----------------------------------------\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1976.35\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1976.76\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.76e+03    |\n",
      "|    ep_rew_mean          | 1950.7273   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1470        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018253382 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.93       |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 233         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00373     |\n",
      "|    value_loss           | 564         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1918.92\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1903.89\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.62e+03    |\n",
      "|    ep_rew_mean          | 1923.0      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1824        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021653278 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.86       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 519         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.00721     |\n",
      "|    value_loss           | 731         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1892.19\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1847.22\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.44e+03    |\n",
      "|    ep_rew_mean          | 1857.081    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 2179        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025603049 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.83       |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 202         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00787     |\n",
      "|    value_loss           | 584         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1854.29\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1867.44\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.41e+03   |\n",
      "|    ep_rew_mean          | 1862.0     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 2534       |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02077505 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.84      |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 254        |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | 0.0076     |\n",
      "|    value_loss           | 688        |\n",
      "----------------------------------------\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1854.36\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1844.37\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.34e+03    |\n",
      "|    ep_rew_mean          | 1844.3654   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 2909        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024324635 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.76       |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 435         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.00917     |\n",
      "|    value_loss           | 835         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1837.82\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1840.29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.28e+03    |\n",
      "|    ep_rew_mean          | 1831.7937   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 3283        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030252375 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 577         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.0063      |\n",
      "|    value_loss           | 1.02e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 84000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1823.72\n",
      "Num timesteps: 88000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1823.01\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.21e+03    |\n",
      "|    ep_rew_mean          | 1815.7261   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 3662        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033117615 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.68       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 216         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.00691     |\n",
      "|    value_loss           | 951         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 92000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1824.35\n",
      "Num timesteps: 96000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1847.09\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.15e+03    |\n",
      "|    ep_rew_mean          | 1831.012    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 4037        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031004556 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.57       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 618         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.00673     |\n",
      "|    value_loss           | 1.37e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1849.33\n",
      "Num timesteps: 104000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1830.34\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.1e+03    |\n",
      "|    ep_rew_mean          | 1836.0737  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 4416       |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03059783 |\n",
      "|    clip_fraction        | 0.363      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.56      |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 458        |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | 0.00776    |\n",
      "|    value_loss           | 1.48e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 108000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1837.88\n",
      "Num timesteps: 112000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1823.30\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 1807.61   |\n",
      "| time/                   |           |\n",
      "|    fps                  | 23        |\n",
      "|    iterations           | 14        |\n",
      "|    time_elapsed         | 4788      |\n",
      "|    total_timesteps      | 114688    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0364666 |\n",
      "|    clip_fraction        | 0.375     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.5      |\n",
      "|    explained_variance   | 0.823     |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 760       |\n",
      "|    n_updates            | 200       |\n",
      "|    policy_gradient_loss | 0.00743   |\n",
      "|    value_loss           | 1.46e+03  |\n",
      "---------------------------------------\n",
      "Num timesteps: 116000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1811.12\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1816.02\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 900         |\n",
      "|    ep_rew_mean          | 1812.04     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 5165        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040540203 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.55       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 522         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.0123      |\n",
      "|    value_loss           | 1.09e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 124000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1827.03\n",
      "Num timesteps: 128000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1806.13\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 856         |\n",
      "|    ep_rew_mean          | 1805.45     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 5545        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034858864 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.46       |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 658         |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    value_loss           | 1.31e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 132000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1804.62\n",
      "Num timesteps: 136000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1817.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 847         |\n",
      "|    ep_rew_mean          | 1847.65     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 5927        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033285253 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 677         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    value_loss           | 1.21e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1854.16\n",
      "Num timesteps: 144000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1811.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 770         |\n",
      "|    ep_rew_mean          | 1808.11     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 6298        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037666496 |\n",
      "|    clip_fraction        | 0.392       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.3        |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 644         |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    value_loss           | 1.59e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 148000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1808.11\n",
      "Num timesteps: 152000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1804.36\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 735         |\n",
      "|    ep_rew_mean          | 1840.88     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 6665        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041620277 |\n",
      "|    clip_fraction        | 0.403       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.32       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 823         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    value_loss           | 1.85e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 156000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1840.88\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1847.54\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 728        |\n",
      "|    ep_rew_mean          | 1857.51    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 7037       |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04294583 |\n",
      "|    clip_fraction        | 0.409      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.33      |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 888        |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | 0.0103     |\n",
      "|    value_loss           | 2.08e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 164000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1857.51\n",
      "Num timesteps: 168000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1866.18\n",
      "Num timesteps: 172000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1880.25\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 740         |\n",
      "|    ep_rew_mean          | 1880.25     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 7403        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040655285 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.36       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 403         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    value_loss           | 1.5e+03     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 176000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1901.42\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1923.97\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 738         |\n",
      "|    ep_rew_mean          | 1923.97     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 7779        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033995815 |\n",
      "|    clip_fraction        | 0.402       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 730         |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.013       |\n",
      "|    value_loss           | 1.84e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 184000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1928.78\n",
      "Num timesteps: 188000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1914.03\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 726         |\n",
      "|    ep_rew_mean          | 1917.61     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 8152        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046047397 |\n",
      "|    clip_fraction        | 0.436       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.13       |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 1.35e+03    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    value_loss           | 2.13e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 192000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1936.73\n",
      "Num timesteps: 196000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 1973.55\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 715        |\n",
      "|    ep_rew_mean          | 1993.1     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 8529       |\n",
      "|    total_timesteps      | 196608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04604076 |\n",
      "|    clip_fraction        | 0.461      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.15      |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 1.24e+03   |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | 0.0151     |\n",
      "|    value_loss           | 1.86e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2009.66\n",
      "Num timesteps: 204000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2029.94\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 700         |\n",
      "|    ep_rew_mean          | 2024.06     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 8903        |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042684045 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5          |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 698         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.013       |\n",
      "|    value_loss           | 1.95e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 208000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2012.40\n",
      "Num timesteps: 212000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2023.89\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 691         |\n",
      "|    ep_rew_mean          | 2025.01     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 9281        |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044160467 |\n",
      "|    clip_fraction        | 0.433       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 1.49e+03    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    value_loss           | 1.81e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 216000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2056.58\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2053.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 714         |\n",
      "|    ep_rew_mean          | 2058.72     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 9661        |\n",
      "|    total_timesteps      | 221184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039360773 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.99       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 732         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | 0.0133      |\n",
      "|    value_loss           | 1.77e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 224000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2061.63\n",
      "Num timesteps: 228000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2095.61\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 760        |\n",
      "|    ep_rew_mean          | 2087.94    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 9991       |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04678555 |\n",
      "|    clip_fraction        | 0.453      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.78      |\n",
      "|    explained_variance   | 0.878      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 1.17e+03   |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | 0.0189     |\n",
      "|    value_loss           | 1.68e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 232000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2084.24\n",
      "Num timesteps: 236000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2071.76\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 752         |\n",
      "|    ep_rew_mean          | 2066.39     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 10300       |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048182037 |\n",
      "|    clip_fraction        | 0.415       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.96       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 589         |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    value_loss           | 1.4e+03     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 240000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2084.94\n",
      "Num timesteps: 244000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2075.09\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 798         |\n",
      "|    ep_rew_mean          | 2075.09     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 10622       |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061164625 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.03       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 489         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.0192      |\n",
      "|    value_loss           | 1.25e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 248000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2088.97\n",
      "Num timesteps: 252000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2118.15\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 874         |\n",
      "|    ep_rew_mean          | 2105.56     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 10934       |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056819715 |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.07       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 0.0232      |\n",
      "|    value_loss           | 448         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 256000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2102.56\n",
      "Num timesteps: 260000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2131.15\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 878         |\n",
      "|    ep_rew_mean          | 2116.39     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 11247       |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044001553 |\n",
      "|    clip_fraction        | 0.386       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 623         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | 0.0151      |\n",
      "|    value_loss           | 1.32e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 264000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2127.30\n",
      "Num timesteps: 268000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2123.74\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 870        |\n",
      "|    ep_rew_mean          | 2143.6     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 11584      |\n",
      "|    total_timesteps      | 270336     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05010645 |\n",
      "|    clip_fraction        | 0.449      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.55      |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 701        |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | 0.0138     |\n",
      "|    value_loss           | 2.22e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 272000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2156.97\n",
      "Num timesteps: 276000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2204.21\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 866         |\n",
      "|    ep_rew_mean          | 2190.15     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 11948       |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049105633 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.47       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 1.24e+03    |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    value_loss           | 2.76e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 280000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2191.53\n",
      "Num timesteps: 284000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2177.72\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 879         |\n",
      "|    ep_rew_mean          | 2174.36     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 12308       |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053566325 |\n",
      "|    clip_fraction        | 0.482       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 986         |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.0156      |\n",
      "|    value_loss           | 2.48e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 288000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2184.79\n",
      "Num timesteps: 292000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2194.46\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 875         |\n",
      "|    ep_rew_mean          | 2194.46     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 12666       |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056587975 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.58       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 934         |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | 0.0178      |\n",
      "|    value_loss           | 1.99e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 296000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2194.46\n",
      "Num timesteps: 300000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2194.46\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 945        |\n",
      "|    ep_rew_mean          | 2219.98    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 23         |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 13028      |\n",
      "|    total_timesteps      | 303104     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07088463 |\n",
      "|    clip_fraction        | 0.502      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.12      |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 3e-05      |\n",
      "|    loss                 | 613        |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | 0.0203     |\n",
      "|    value_loss           | 1.67e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 304000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2221.29\n",
      "Num timesteps: 308000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2238.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 945         |\n",
      "|    ep_rew_mean          | 2261.55     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 13363       |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076534234 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 44.7        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.0269      |\n",
      "|    value_loss           | 278         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 312000\n",
      "Best mean reward: 2245.33 - Last mean reward per episode: 2261.55\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 316000\n",
      "Best mean reward: 2261.55 - Last mean reward per episode: 2256.27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/supermario/TrainAgent.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----START LEARNING----\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m callback \u001b[39m=\u001b[39m SaveOnBestTrainingRewardCallback(check_freq\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, log_dir\u001b[39m=\u001b[39mlog_dir)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1000000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback, tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPPO-00003\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m model\u001b[39m.\u001b[39msave(env_id)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----DONE LEARNING----\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:169\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     obs_tensor \u001b[39m=\u001b[39m obs_as_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 169\u001b[0m     actions, values, log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(obs_tensor)\n\u001b[1;32m    170\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    172\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/common/policies.py:617\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[39m# Preprocess the observation if needed\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(obs)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_features_extractor:\n\u001b[1;32m    619\u001b[0m     latent_pi, latent_vf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/common/policies.py:640\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[39mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \n\u001b[1;32m    636\u001b[0m \u001b[39m:param obs: Observation\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[39m:return: the output of the features extractor(s)\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 640\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mextract_features(obs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor)\n\u001b[1;32m    641\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m     pi_features \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi_features_extractor)\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/common/policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m :return: The extracted features\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m preprocessed_obs \u001b[39m=\u001b[39m preprocess_obs(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space, normalize_images\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_images)\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/common/torch_layers.py:106\u001b[0m, in \u001b[0;36mNatureCNN.forward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observations: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnn(observations))\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecMonitor, SubprocVecEnv\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "\n",
    "import os\n",
    "import retro\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "def make_env(env_id, rank, seed=0):\n",
    "    def _init():\n",
    "        env = retro.make(game=env_id)\n",
    "        env = MaxAndSkipEnv(env, 4)\n",
    "        # env.seed(seed + rank) # seed doesn't want to work... not sure why\n",
    "        return env\n",
    "\n",
    "    # set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "log_dir = 'log/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env_id = \"SuperMarioBros-Nes\"\n",
    "num_cpu = 4\n",
    "\n",
    "\n",
    "env = VecMonitor(SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)]), log_dir+\"monitor\")\n",
    "#model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=\"./board/\", learning_rate=0.00003)\n",
    "\n",
    "model = PPO.load(\"./log/best_model.zip\", env=env)\n",
    "print(\"----START LEARNING----\")\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "model.learn(total_timesteps=1000000, callback=callback, tb_log_name=\"PPO-00003\")\n",
    "model.save(env_id)\n",
    "print(\"----DONE LEARNING----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
