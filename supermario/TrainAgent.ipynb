{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 03:46:06.291049: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 03:46:06.331179: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 03:46:06.335883: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 03:46:06.336264: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "----START LEARNING----\n",
      "Logging to ./board/PPO-00003_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 253  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 32   |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -inf - Last mean reward per episode: 391.00\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 391.00 - Last mean reward per episode: 391.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.61e+03    |\n",
      "|    ep_rew_mean          | 391.0       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 379         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011534795 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.23       |\n",
      "|    explained_variance   | 0.00204     |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 9.52        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    value_loss           | 29          |\n",
      "-----------------------------------------\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 391.00 - Last mean reward per episode: 956.50\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 956.50 - Last mean reward per episode: 1093.50\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.02e+03    |\n",
      "|    ep_rew_mean          | 1093.5      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 33          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 732         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011420062 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 41.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00937    |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 1093.50 - Last mean reward per episode: 1262.20\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 1262.20 - Last mean reward per episode: 1262.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.59e+03    |\n",
      "|    ep_rew_mean          | 1262.2      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 30          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1065        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009933438 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.18       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 45.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.000677   |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 1262.20 - Last mean reward per episode: 1181.71\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 1262.20 - Last mean reward per episode: 1181.71\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.11e+03    |\n",
      "|    ep_rew_mean          | 1181.7142   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1376        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011931597 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 16.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000376   |\n",
      "|    value_loss           | 54.5        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 1262.20 - Last mean reward per episode: 1181.71\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 1262.20 - Last mean reward per episode: 1320.50\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 4.46e+03    |\n",
      "|    ep_rew_mean          | 1478.3      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1688        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012229127 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.15       |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00163     |\n",
      "|    value_loss           | 99.9        |\n",
      "-----------------------------------------\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 1320.50 - Last mean reward per episode: 1442.08\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 1442.08 - Last mean reward per episode: 1405.27\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.51e+03    |\n",
      "|    ep_rew_mean          | 1405.2667   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 28          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 2011        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014094834 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.12       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 64          |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000533   |\n",
      "|    value_loss           | 151         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 1442.08 - Last mean reward per episode: 1438.44\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 1442.08 - Last mean reward per episode: 1512.45\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.09e+03    |\n",
      "|    ep_rew_mean          | 1512.45     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 2340        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014504004 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -6.08       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00127     |\n",
      "|    value_loss           | 388         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/supermario/TrainAgent.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----START LEARNING----\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m callback \u001b[39m=\u001b[39m SaveOnBestTrainingRewardCallback(check_freq\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, log_dir\u001b[39m=\u001b[39mlog_dir)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1000000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback, tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPPO-00003\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m model\u001b[39m.\u001b[39msave(env_id)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6d70617373696f6e6174655f747572696e67227d/home/jovyan/work/supermario/TrainAgent.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----DONE LEARNING----\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:281\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    283\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:272\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m# Optimization step\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 272\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    273\u001b[0m \u001b[39m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    274\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/supermarioEnv/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecMonitor, SubprocVecEnv\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "\n",
    "import os\n",
    "import retro\n",
    "\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "def make_env(env_id, rank, seed=0):\n",
    "    def _init():\n",
    "        env = retro.make(game=env_id)\n",
    "        env = MaxAndSkipEnv(env, 4)\n",
    "        # env.seed(seed + rank) # seed doesn't want to work... not sure why\n",
    "        return env\n",
    "\n",
    "    # set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "log_dir = 'log/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env_id = \"SuperMarioBros-Nes\"\n",
    "num_cpu = 4\n",
    "\n",
    "\n",
    "env = VecMonitor(SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)]), log_dir+\"monitor\")\n",
    "#model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=\"./board/\", learning_rate=0.00003)\n",
    "\n",
    "model = PPO.load(\"./log/best_model.zip\", env=env)\n",
    "print(\"----START LEARNING----\")\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "model.learn(total_timesteps=1000000, callback=callback, tb_log_name=\"PPO-00003\")\n",
    "model.save(env_id)\n",
    "print(\"----DONE LEARNING----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
